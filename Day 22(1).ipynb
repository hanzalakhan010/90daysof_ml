{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6261877,"sourceType":"datasetVersion","datasetId":3599110}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Your Learning Summary**  \nHere’s a concise breakdown of what you’ve learned through this process, structured around key concepts and practical fixes:\n\n---\n\n### **1. Critical Architecture Fixes**  \n- **Layer Order**:  \n  - Flatten 3D outputs of Conv layers **before** Dense layers.  \n- **Activation Functions**:  \n  - Use `sigmoid` for **binary classification** (not `softmax`).  \n- **Regularization**:  \n  - Added **Dropout** (e.g., 0.5 after Dense layers) and **L2 regularization** to prevent overfitting.  \n- **Model Depth**:  \n  - Increased layers (e.g., 3 Conv2D + 2 Dense) for better feature extraction.  \n\n---\n\n### **2. Data Preprocessing**  \n- **Normalization**: Scaled pixel values to `[0, 1]` using `Rescaling(1./255)`.  \n- **Input Shape**: Ensured consistency (e.g., `(50,50,3)` for RGB images).  \n- **Class Balance**: Checked for equal dog/cat samples or used `class_weight` for imbalance.  \n\n---\n\n### **3. Training Improvements**  \n- **Learning Rate**: Started with a small value (e.g., `0.0001`) for stable training.  \n- **Early Stopping**: Halted training when validation loss plateaued (`patience=5`).  \n- **Data Augmentation**: Applied rotations/flips to artificially expand the dataset.  \n\n---\n\n### **4. Debugging & Validation**  \n- **Visualized Predictions**: Confirmed the model isn’t guessing randomly.  \n- **Tracked Metrics**: Monitored both loss and accuracy (training vs. validation).  \n- **Avoided Overfitting**: Used dropout, L2 regularization, and validation splits.  \n\n---\n\n### **5. Advanced Techniques (Optional)**  \n- **Transfer Learning**: Pre-trained models (e.g., VGG16) for better feature extraction.  \n- **Hyperparameter Tuning**: Adjusted dropout rates, learning rate, and batch size.  \n\n---\n\n### **Key Takeaways**  \n1. **Architecture Matters**:  \n   - CNNs need proper layer ordering (Conv → Pool → Flatten → Dense).  \n   - Activation functions must align with the task (`sigmoid` for binary).  \n2. **Data is King**:  \n   - Normalization, augmentation, and balancing are critical.  \n3. **Regularization is Non-Negotiable**:  \n   - Dropout and L2 prevent memorization of noise.  \n4. **Iterative Improvement**:  \n   - Start simple → Debug → Add complexity → Validate.  \n\n---\n\n### **Real-World Relevance**  \nYou’ve learned how to:  \n- Diagnose and fix **model architecture errors**.  \n- Preprocess data for **robust training**.  \n- Use regularization to **generalize to new data**.  \n- Deploy techniques like early stopping to **save time/resources**.  \n\nGreat progress! 🚀 Let me know if you want to tackle another challenge (e.g., deployment, hyperparameter tuning).","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        ...\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:04:43.476743Z","iopub.execute_input":"2025-01-26T22:04:43.477028Z","iopub.status.idle":"2025-01-26T22:05:42.305322Z","shell.execute_reply.started":"2025-01-26T22:04:43.477005Z","shell.execute_reply":"2025-01-26T22:05:42.304246Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install opencv-python\n!pip install tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-01-26T22:09:07.171631Z","shell.execute_reply.started":"2025-01-26T22:08:57.986567Z","shell.execute_reply":"2025-01-26T22:09:07.170632Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport random\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:09:10.136696Z","iopub.execute_input":"2025-01-26T22:09:10.137066Z","iopub.status.idle":"2025-01-26T22:09:24.259381Z","shell.execute_reply.started":"2025-01-26T22:09:10.137039Z","shell.execute_reply":"2025-01-26T22:09:24.258297Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import normalize\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:04:02.288084Z","iopub.execute_input":"2025-01-26T23:04:02.288423Z","iopub.status.idle":"2025-01-26T23:04:02.293039Z","shell.execute_reply.started":"2025-01-26T23:04:02.288389Z","shell.execute_reply":"2025-01-26T23:04:02.291932Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"DATADIR = '/kaggle/input/dogs-vs-cats/dataset/train'\nDATADIR2 = '/kaggle/input/dogs-vs-cats/dataset/test'\ncategories = ['cats',\"dogs\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:11:13.062934Z","iopub.execute_input":"2025-01-26T22:11:13.063302Z","iopub.status.idle":"2025-01-26T22:11:13.067833Z","shell.execute_reply.started":"2025-01-26T22:11:13.063276Z","shell.execute_reply":"2025-01-26T22:11:13.066724Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"img_size = 50\ntraining_data = []\nfor category in categories:\n    path = os.path.join(DATADIR,category)\n    for img in os.listdir(path):\n        class_num = categories.index(category)\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        new_array = cv2.resize(img_array,(img_size,img_size))\n        training_data.append([new_array,class_num])\n    path = os.path.join(DATADIR2,category)\n    for img in os.listdir(path):\n        class_num = categories.index(category)\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        new_array = cv2.resize(img_array,(img_size,img_size))\n        training_data.append([new_array,class_num])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:11:15.425615Z","iopub.execute_input":"2025-01-26T22:11:15.425969Z","iopub.status.idle":"2025-01-26T22:15:02.329204Z","shell.execute_reply.started":"2025-01-26T22:11:15.425942Z","shell.execute_reply":"2025-01-26T22:15:02.328406Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"random.shuffle(training_data)\n# NAME = \"Cats-vs-dogs-64x2-CNN(4)\"\n# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:45:09.514207Z","iopub.execute_input":"2025-01-26T22:45:09.514548Z","iopub.status.idle":"2025-01-26T22:45:09.535487Z","shell.execute_reply.started":"2025-01-26T22:45:09.514518Z","shell.execute_reply":"2025-01-26T22:45:09.534504Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"X = []\ny = []\nfor feature,label in training_data:\n    X.append(feature)\n    y.append(label)\n\nX =  np.array(X).reshape(-1,img_size,img_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:18:28.390857Z","iopub.execute_input":"2025-01-26T22:18:28.391200Z","iopub.status.idle":"2025-01-26T22:18:28.471906Z","shell.execute_reply.started":"2025-01-26T22:18:28.391170Z","shell.execute_reply":"2025-01-26T22:18:28.471146Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# pickle_out = open('X.pickle','wb')\n# pickle.dump(X,pickle_out)\n# pickle_out.close\n# pickle_out = open('y.pickle','wb')\n# pickle.dump(y,pickle_out)\n# pickle_out.close\n\n# with open(open('X.pickle','rb')) as inp:\n#     X = pickle.load(inp)\n\n# with open(open('y.pickle','rb')) as out:\n#     X = pickle.load(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:18:01.719087Z","iopub.execute_input":"2025-01-26T07:18:01.719419Z","iopub.status.idle":"2025-01-26T07:18:01.869770Z","shell.execute_reply.started":"2025-01-26T07:18:01.719391Z","shell.execute_reply":"2025-01-26T07:18:01.869106Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<function BufferedWriter.close>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size =.2,random_state = 34)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\ny = np.array(y)\nx_train = normalize(x_train)\nx_test = normalize(x_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:53:34.256440Z","iopub.execute_input":"2025-01-26T22:53:34.256858Z","iopub.status.idle":"2025-01-26T22:53:34.916498Z","shell.execute_reply.started":"2025-01-26T22:53:34.256828Z","shell.execute_reply":"2025-01-26T22:53:34.915696Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor='val_loss',   # Metric to monitor (usually validation loss/accuracy)\n    patience=5,           # Number of epochs to wait before stopping\n    restore_best_weights=True,  # Restore model weights from the epoch with best performance\n    mode='min',           # Direction to monitor: 'min' for loss, 'max' for accuracy\n    verbose=1             # Show messages when stopping\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:04:10.745593Z","iopub.execute_input":"2025-01-26T23:04:10.745943Z","iopub.status.idle":"2025-01-26T23:04:10.750077Z","shell.execute_reply.started":"2025-01-26T23:04:10.745913Z","shell.execute_reply":"2025-01-26T23:04:10.749243Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"model = Sequential(\n    [\n        Conv2D(64,(3,3),input_shape = (50,50,1),activation ='relu'),\n        MaxPooling2D(pool_size = (2,2)),\n        Conv2D(64,(3,3),activation ='relu'),\n        MaxPooling2D(pool_size = (2,2)),\n        Flatten(),\n        Dense(128,activation='relu', kernel_regularizer=regularizers.L2(0.001)),\n        Dense(128,activation='relu', kernel_regularizer=regularizers.L2(0.01)),\n        Dense(1,activation = 'sigmoid')\n    ]\n)\n\n# model.add(Conv2D(64,(3,3),input_shape =(50,50,1)))\n# model.add(Activation('relu'))\n# model.add(MaxPooling2D(pool_size = (2,2)))\n\n# model.add(Conv2D(64,(3,3)))\n# model.add(Activation('relu'))\n# model.add(Dropout(.5))\n# model.add(MaxPooling2D(pool_size = (3,3)))\n\n# model.add(Conv2D(64,(3,3)))\n# model.add(Activation('relu'))\n# model.add(Dropout(.2))\n# model.add(MaxPooling2D(pool_size = (3,3)))\n\n# model.add(Dense(128,activation='relu', kernel_regularizer=regularizers.L2(0.01)))\n# # model.add(Activation('relu'))\n\n\n# model.add(Flatten())\n# model.add(Dense(1))\n# model.add(Activation('relu'))\n\nmodel.compile(\n    loss = 'binary_crossentropy',\n    optimizer = Adam(learning_rate = .001),\n    metrics = ['accuracy']\n)\n# model.fit(x_train,y_train,epochs = 10,callbacks = [early_stopping],validation_data=(x_test,y_test))\nmodel.fit(x_train,y_train,epochs = 10,validation_data=(x_test,y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:21:26.958156Z","iopub.execute_input":"2025-01-26T23:21:26.958676Z","iopub.status.idle":"2025-01-26T23:22:08.420598Z","shell.execute_reply.started":"2025-01-26T23:21:26.958623Z","shell.execute_reply":"2025-01-26T23:22:08.419717Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5220 - loss: 0.9642 - val_accuracy: 0.6088 - val_loss: 0.6724\nEpoch 2/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6255 - loss: 0.6595 - val_accuracy: 0.6929 - val_loss: 0.6034\nEpoch 3/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6960 - loss: 0.5979 - val_accuracy: 0.7283 - val_loss: 0.5646\nEpoch 4/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7218 - loss: 0.5657 - val_accuracy: 0.7259 - val_loss: 0.5665\nEpoch 5/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.5416 - val_accuracy: 0.7505 - val_loss: 0.5326\nEpoch 6/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7643 - loss: 0.5169 - val_accuracy: 0.7676 - val_loss: 0.5125\nEpoch 7/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7822 - loss: 0.4882 - val_accuracy: 0.7721 - val_loss: 0.5085\nEpoch 8/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7824 - loss: 0.4828 - val_accuracy: 0.7684 - val_loss: 0.5063\nEpoch 9/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7949 - loss: 0.4697 - val_accuracy: 0.7747 - val_loss: 0.4966\nEpoch 10/10\n\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7985 - loss: 0.4624 - val_accuracy: 0.7805 - val_loss: 0.4912\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7faaa0bd58d0>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}